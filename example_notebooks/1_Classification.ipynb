{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many datasets, learning and assessing classifiers requires 3-4 lines of code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics, datasets, tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. load and partition data\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7)\n",
    "\n",
    "# 2. learn classifier\n",
    "predictor = tree.DecisionTreeClassifier(max_depth=3)\n",
    "predictor.fit(X_train, y_train)\n",
    "\n",
    "# 3. plot classifier\n",
    "figure = plt.figure(figsize=(12, 6))\n",
    "tree.plot_tree(predictor, feature_names=iris.feature_names, class_names=iris.target_names, impurity=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. test classifier\n",
    "y_pred = predictor.predict(X_test)\n",
    "print(\"accuracy on testing set:\",  round(metrics.accuracy_score(y_test, y_pred),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_pred = predictor.predict([X_test[0]])\n",
    "print(\"prediction for instance\",X_test[0],\"is\",iris.target_names[single_pred[0]],\"was suppose to be\",iris.target_names[y_test[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nevertheless, many datasets are not well prepared for the straightforward learning of classifiers and thus require **preprocessing**.<br>\n",
    "**Major steps**: missing imputation (see previous notebook), scaling of numeric vars, optional dummification of categorical vars (mandatory in *sklearn*)<br>\n",
    "**Others**: data balancing, discretization of numeric vars, aggregation of categorical vars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Loading** data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "\n",
    "# read titanic data\n",
    "df = pd.read_csv('titanic.csv')\n",
    "\n",
    "# remove non relevant variables\n",
    "df.drop([\"PassengerId\",\"Name\",\"Ticket\",\"Cabin\"], axis = 1, inplace=True)\n",
    "\n",
    "# remove observations with missings\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Scaling** of numeric variables<br>Optional step, yet relevant for some predictive models (e.g., kNN)<br>Be careful: for classification purposes, scalers and imputers should be learned using training data only (see data sampling section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric variables [0,1]: problem with Fare variable?\n",
    "df_scaled = df.copy()\n",
    "df_scaled[['Age','Fare']] = MinMaxScaler().fit_transform(df[['Age','Fare']])\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numeric variables as N(0,1): minimizes Fare drawbacks\n",
    "df_scaled[['Age','Fare']] = StandardScaler().fit_transform(df[['Age','Fare']])\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c) Aggregation** (*Parch* ordinal variable can be optionally aggregated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Parch'] = pd.to_numeric(df['Parch'])\n",
    "df.value_counts(\"Parch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['Parch']] = df[['Parch']].apply(lambda x: [y if y < 2 else 3 for y in x])\n",
    "df.value_counts(\"Parch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**d) Dummification** of categorical variables<br>Dummification is *optional* for binary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# select columns to dummify\n",
    "vars_to_dummify = ['Pclass','Parch','Embarked']\n",
    "X = df[vars_to_dummify]\n",
    "\n",
    "# dummify columns\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore') #handle_unknown='ignore', sparse=False, dtype=bool)\n",
    "trans_X = encoder.fit_transform(X)\n",
    "new_vars = encoder.get_feature_names(vars_to_dummify)\n",
    "new_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess Age binary variable\n",
    "df_dummified = df.copy()\n",
    "df_dummified.replace({'female':0, 'male':1}, inplace=True)\n",
    "\n",
    "# append dummified columns\n",
    "df_dummified.drop(vars_to_dummify, axis = 1, inplace=True)\n",
    "df_dummified[new_vars] = pd.DataFrame(trans_X, columns=new_vars)\n",
    "df_dummified.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Data sampling strategies for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate input from output data\n",
    "X = df_dummified.drop('Survived', axis=1)\n",
    "y = df_dummified['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hold-out strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us perform a simple data partition into train and test data (*train_test_split*) to learn and assess a simple decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *stratify* parameter in *train_test_split* ensures identical class distribution in train and test sets<br>\n",
    "- optional *random_state* parameter can be used to fix partition to replicate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, stratify=y)\n",
    "print(\"#training obs =\",len(X_train),\"\\n#testing obs =\",len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn classifier using hold-out partitioning\n",
    "predictor = DecisionTreeClassifier()\n",
    "predictor.fit(X_train, y_train)\n",
    "y_pred = predictor.predict(X_test)\n",
    "print(\"Reference survivals:\\n\",y_test.tolist(),\"\\n\\nPredicted survivals:\\n\",y_pred.tolist())\n",
    "print(\"\\nPredictor accuracy:\",  round(metrics.accuracy_score(y_test, y_pred),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation offers a more comprehensive evaluation, multiple estimates (variability, statistical tests)<br>\n",
    "Two possibilities: a) predefined cross_eval_score utilities, b) manual fold iteration (useful for dedicated preprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a) predefined capabilities\n",
    "\n",
    "print(\"Fold accuracies:\\n\",cross_val_score(predictor, X, y, cv=10, scoring='accuracy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b) manual fold iteration\n",
    "\n",
    "acc_folds = []\n",
    "folds = StratifiedKFold(n_splits=10)\n",
    "\n",
    "# iterate per fold\n",
    "for train_k, test_k in folds.split(X, y):\n",
    "    X_train, X_test = X.iloc[train_k], X.iloc[test_k]\n",
    "    y_train, y_test = y.iloc[train_k], y.iloc[test_k]\n",
    "    \n",
    "    # train and assess\n",
    "    predictor.fit(X_train, y_train)\n",
    "    y_pred = predictor.predict(X_test)\n",
    "    acc_folds.append(round(metrics.accuracy_score(y_test, y_pred),2))\n",
    "    \n",
    "print(\"Fold accuracies:\", acc_folds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section provides a *high-level view* on how to instantiate different classification models.<br>\n",
    "Please consult **sklearn documentation** for details on the parameterization and visualization of these predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**a) Simple comparison** on the prepared dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "classifiers = [\n",
    "    (\"Decision Tree\", DecisionTreeClassifier(max_depth=5)),\n",
    "    (\"kNN\", KNeighborsClassifier(n_neighbors=3)),\n",
    "    (\"Naive Bayes\", GaussianNB()),\n",
    "    (\"Neural Network\", MLPClassifier(alpha=1, max_iter=1000)),\n",
    "    (\"Linear SVM\", SVC(kernel=\"linear\", C=0.025))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "def warn(*args, **kwargs): pass\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, classifier in classifiers:\n",
    "    accs = cross_val_score(classifier, X, y, cv=10, scoring='accuracy')\n",
    "    print(name, \"accuracy =\", round(np.mean(accs),2), \"±\", round(np.std(accs),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b) Visualizing boundaries** in bivariate numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "\n",
    "# generate three bivariate datasets\n",
    "datasets = [\n",
    "    make_moons(noise=0.3, random_state=0),\n",
    "    make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "    make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1)\n",
    "]\n",
    "\n",
    "# plot the datasets\n",
    "figure = plt.figure(figsize=(12, 4))\n",
    "i = 1\n",
    "for ds in datasets:\n",
    "    X, y = ds\n",
    "    ax = plt.subplot(1, 3, i)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=ListedColormap(['#FF0000', '#0000FF']))\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Goal: draw the classification boundaries\n",
    "i, colors = 1, ListedColormap(['#FF0000', '#0000FF'])\n",
    "figure = plt.figure(figsize=(18, 9))\n",
    "\n",
    "# 1. iterate over datasets\n",
    "for ds in datasets:\n",
    "    X, y = ds\n",
    "    \n",
    "    # 1a. plot the dataset\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=colors)\n",
    "    i += 1\n",
    "\n",
    "    # 1b. split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\n",
    "    \n",
    "    # 1c. record plot limits\n",
    "    x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "    y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, .02), np.arange(y_min, y_max, .02))\n",
    "\n",
    " \n",
    "    # 2. iterate over classifiers\n",
    "    for name, clf in classifiers:\n",
    "        \n",
    "        # 2a. learn and apply classifier\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        \n",
    "        # 2b. plot classification boundary\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "        if hasattr(clf, \"decision_function\"): Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        else: Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        ax.contourf(xx, yy, Z, cmap=plt.cm.RdBu, alpha=.8)\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=y, cmap=colors)\n",
    "        ax.set_xlim(xx.min(), xx.max())\n",
    "        ax.set_ylim(yy.min(), yy.max())\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        ax.set_title(name)\n",
    "        ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'), size=15, horizontalalignment='right')\n",
    "        i += 1\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
